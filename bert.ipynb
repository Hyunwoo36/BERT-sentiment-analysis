{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Sentiment analysis\n",
    "### - using stanford AI imdb review data (https://ai.stanford.edu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84125825/84125825 [==============================] - 19s 0us/step\n"
     ]
    }
   ],
   "source": [
    "current_folder = os.getcwd()\n",
    " \n",
    "dataset = tf.keras.utils.get_file(\n",
    "    fname =\"aclImdb.tar.gz\", \n",
    "    origin =\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "    cache_dir=  current_folder,\n",
    "    extract = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aclImdb', 'aclImdb.tar.gz']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = os.path.dirname(dataset)\n",
    "# Check the dataset\n",
    "os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset directory\n",
    "dataset_dir = os.path.join(dataset_path, 'aclImdb')\n",
    " \n",
    "# Check the Dataset directory\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsup',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir,'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat: 9 0:9 1:1 2:4 3:4 4:6 5:4 6:2 7:2 8:4 10:4 12:2 26:1 27:1 28:1 29:2 32:1 41:1 45:1 47:1 50:1 54:2 57:1 59:1 63:2 64:1 66:1 68:2 70:1 72:1 78:1 100:1 106:1 116:1 122:1 125:1 136:1 140:1 142:1 150:1 167:1 183:1 201:1 207:1 208:1 213:1 217:1 230:1 255:1 321:5 343:1 357:1 370:1 390:2 468:1 514:1 571:1 619:1 671:1 766:1 877:1 1057:1 1179:1 1192:1 1402:2 1416:1 1477:2 1940:1 1941:1 2096:1 2243:1 2285:1 2379:1 2934:1 2938:1 3520:1 3647:1 4938:1 5138:4 5715:1 5726:1 5731:1 5812:1 8319:1 8567:1 10480:1 14239:1 20604:1 22409:4 24551:1 47304:1\n",
      "neg: c:\\Users\\harry\\Desktop\\projects\\mlProjects\\datasets\\aclImdb\\train\\neg\n",
      "pos: c:\\Users\\harry\\Desktop\\projects\\mlProjects\\datasets\\aclImdb\\train\\pos\n",
      "unsup: c:\\Users\\harry\\Desktop\\projects\\mlProjects\\datasets\\aclImdb\\train\\unsup\n",
      "unsupBow.feat: 0 0:8 1:6 3:5 4:2 5:1 7:1 8:5 9:2 10:1 11:2 13:3 16:1 17:1 18:1 19:1 22:3 24:1 26:3 28:1 30:1 31:1 35:2 36:1 39:2 40:1 41:2 46:2 47:1 48:1 52:1 63:1 67:1 68:1 74:1 81:1 83:1 87:1 104:1 105:1 112:1 117:1 131:1 151:1 155:1 170:1 198:1 225:1 226:1 288:2 291:1 320:1 331:1 342:1 364:1 374:1 384:2 385:1 407:1 437:1 441:1 465:1 468:1 470:1 519:1 595:1 615:1 650:1 692:1 851:1 937:1 940:1 1100:1 1264:1 1297:1 1317:1 1514:1 1728:1 1793:1 1948:1 2088:1 2257:1 2358:1 2584:2 2645:1 2735:1 3050:1 4297:1 5385:1 5858:1 7382:1 7767:1 7773:1 9306:1 10413:1 11881:1 15907:1 18613:1 18877:1 25479:1\n",
      "urls_neg.txt: http://www.imdb.com/title/tt0064354/usercomments\n",
      "urls_pos.txt: http://www.imdb.com/title/tt0453418/usercomments\n",
      "urls_unsup.txt: http://www.imdb.com/title/tt0018515/usercomments\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(train_dir):\n",
    "    file_path = os.path.join(train_dir, file)\n",
    "    # Check if it's a file (not a directory)\n",
    "    if os.path.isfile(file_path): \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            first_value = f.readline().strip()\n",
    "            print(f\"{file}: {first_value}\")\n",
    "    else:\n",
    "        print(f\"{file}: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory):\n",
    "    data = {\"sentence\": [], \"sentiment\": []}\n",
    "    for file_name in os.listdir(directory):\n",
    "        print(file_name)\n",
    "        if file_name == 'pos':\n",
    "            positive_dir = os.path.join(directory, file_name)\n",
    "            for text_file in os.listdir(positive_dir):\n",
    "                text = os.path.join(positive_dir, text_file)\n",
    "                with open(text, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data[\"sentence\"].append(f.read())\n",
    "                    data[\"sentiment\"].append(1)\n",
    "        elif file_name == 'neg':\n",
    "            negative_dir = os.path.join(directory, file_name)\n",
    "            for text_file in os.listdir(negative_dir):\n",
    "                text = os.path.join(negative_dir, text_file)\n",
    "                with open(text, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data[\"sentence\"].append(f.read())\n",
    "                    data[\"sentiment\"].append(0)\n",
    "             \n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat\n",
      "neg\n",
      "pos\n",
      "unsup\n",
      "unsupBow.feat\n",
      "urls_neg.txt\n",
      "urls_pos.txt\n",
      "urls_unsup.txt\n",
      "                                            sentence  sentiment\n",
      "0  Story of a man who has unnatural feelings for ...          0\n",
      "1  Airport '77 starts as a brand new luxury 747 p...          0\n",
      "2  This film lacked something I couldn't put my f...          0\n",
      "3  Sorry everyone,,, I know this is supposed to b...          0\n",
      "4  When I was little my parents took me along to ...          0\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from the train_dir\n",
    "train_df = load_dataset(train_dir)\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat\n",
      "neg\n",
      "pos\n",
      "urls_neg.txt\n",
      "urls_pos.txt\n",
      "                                            sentence  sentiment\n",
      "0  Once again Mr. Costner has dragged out a movie...          0\n",
      "1  This is an example of why the majority of acti...          0\n",
      "2  First of all I hate those moronic rappers, who...          0\n",
      "3  Not even the Beatles could write songs everyon...          0\n",
      "4  Brass pictures (movies is not a fitting word f...          0\n"
     ]
    }
   ],
   "source": [
    "test_dir = os.path.join(dataset_dir,'test')\n",
    " \n",
    "# Load the dataset from the train_dir\n",
    "test_df = load_dataset(test_dir)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = train_df['sentiment'].value_counts()\n",
    " \n",
    "fig =px.bar(x= {0:'Negative',1:'Positive'},\n",
    "            y= sentiment_counts.values,\n",
    "            color=sentiment_counts.index,\n",
    "            color_discrete_sequence =  px.colors.qualitative.Dark24,\n",
    "            title='<b>Sentiments Counts')\n",
    " \n",
    "fig.update_layout(title='Sentiments Counts',\n",
    "                  xaxis_title='Sentiment',\n",
    "                  yaxis_title='Counts',\n",
    "                  template='plotly_dark')\n",
    " \n",
    "# Show the bar chart\n",
    "fig.show()\n",
    "pyo.plot(fig, filename = 'Sentiments Counts.html', auto_open = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = re.sub(r'\\[[^]]*\\]', '', soup.get_text())\n",
    "    pattern = r\"[^a-zA-Z0-9\\s,']\"\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_df['Cleaned_sentence'] = train_df['sentence'].apply(text_cleaning).tolist()\n",
    "# Test dataset\n",
    "test_df['Cleaned_sentence'] = test_df['sentence'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate word cloud\n",
    "def generate_wordcloud(text,Title):\n",
    "    all_text = \" \".join(text)\n",
    "    wordcloud = WordCloud(width=800, \n",
    "                          height=400,\n",
    "                          stopwords=set(STOPWORDS), \n",
    "                          background_color='black').generate(all_text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(Title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive reviews\n",
    "positive = train_df[train_df['sentiment']==1]['Cleaned_sentence'].tolist()\n",
    "generate_wordcloud(positive,'Positive Review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative reviews\n",
    "negative = train_df[train_df['sentiment']==0]['Cleaned_sentence'].tolist()\n",
    "generate_wordcloud(negative,'Negative Review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "#Reviews = \"[CLS] \" +train_df['Cleaned_sentence'] + \"[SEP]\"\n",
    "Reviews = train_df['Cleaned_sentence']\n",
    "Target = train_df['sentiment']\n",
    " \n",
    "# Test data\n",
    "#test_reviews =  \"[CLS] \" +test_df['Cleaned_sentence'] + \"[SEP]\"\n",
    "test_reviews = test_df['Cleaned_sentence']\n",
    "test_targets = test_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, x_test, y_val, y_test = train_test_split(test_reviews,\n",
    "                                                    test_targets,\n",
    "                                                    test_size=0.5, \n",
    "                                                    stratify = test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize and encode the data using the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len= 128\n",
    "# Tokenize and encode the sentences\n",
    "X_train_encoded = tokenizer.batch_encode_plus(Reviews.tolist(),\n",
    "                                              padding=True, \n",
    "                                              truncation=True,\n",
    "                                              max_length = max_len,\n",
    "                                              return_tensors='tf')\n",
    " \n",
    "X_val_encoded = tokenizer.batch_encode_plus(x_val.tolist(), \n",
    "                                              padding=True, \n",
    "                                              truncation=True,\n",
    "                                              max_length = max_len,\n",
    "                                              return_tensors='tf')\n",
    " \n",
    "X_test_encoded = tokenizer.batch_encode_plus(x_test.tolist(), \n",
    "                                              padding=True, \n",
    "                                              truncation=True,\n",
    "                                              max_length = max_len,\n",
    "                                              return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "print('Training Comments -->>',Reviews[k])\n",
    "print('\\nInput Ids -->>\\n',X_train_encoded['input_ids'][k])\n",
    "print('\\nDecoded Ids -->>\\n',tokenizer.decode(X_train_encoded['input_ids'][k]))\n",
    "print('\\nAttention Mask -->>\\n',X_train_encoded['attention_mask'][k])\n",
    "print('\\nLabels -->>',Target[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize the model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with an appropriate optimizer, loss function, and metrics\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "\t[X_train_encoded['input_ids'], X_train_encoded['token_type_ids'], X_train_encoded['attention_mask']],\n",
    "\tTarget,\n",
    "\tvalidation_data=(\n",
    "\t[X_val_encoded['input_ids'], X_val_encoded['token_type_ids'], X_val_encoded['attention_mask']],y_val),\n",
    "\tbatch_size=32,\n",
    "\tepochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "\t[X_test_encoded['input_ids'], X_test_encoded['token_type_ids'], X_test_encoded['attention_mask']],\n",
    "\ty_test\n",
    ")\n",
    "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'path-to-save'\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(path +'/Tokenizer')\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(path +'/Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(path +'/Tokenizer')\n",
    "\n",
    "# Load model\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(path +'/Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = bert_model.predict(\n",
    "\t[X_test_encoded['input_ids'], X_test_encoded['token_type_ids'], X_test_encoded['attention_mask']])\n",
    "\n",
    "# pred is of type TFSequenceClassifierOutput\n",
    "logits = pred.logits\n",
    "\n",
    "# Use argmax along the appropriate axis to get the predicted labels\n",
    "pred_labels = tf.argmax(logits, axis=1)\n",
    "\n",
    "# Convert the predicted labels to a NumPy array\n",
    "pred_labels = pred_labels.numpy()\n",
    "\n",
    "label = {\n",
    "\t1: 'positive',\n",
    "\t0: 'Negative'\n",
    "}\n",
    "\n",
    "# Map the predicted labels to their corresponding strings using the label dictionary\n",
    "pred_labels = [label[i] for i in pred_labels]\n",
    "Actual = [label[i] for i in y_test]\n",
    "\n",
    "print('Predicted Label :', pred_labels[:10])\n",
    "print('Actual Label :', Actual[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report: \\n\", classification_report(Actual, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_sentiment(Review, Tokenizer=bert_tokenizer, Model=bert_model):\n",
    "\t# Convert Review to a list if it's not already a list\n",
    "\tif not isinstance(Review, list):\n",
    "\t\tReview = [Review]\n",
    "\n",
    "\tInput_ids, Token_type_ids, Attention_mask = Tokenizer.batch_encode_plus(Review,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpadding=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmax_length=128,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treturn_tensors='tf').values()\n",
    "\tprediction = Model.predict([Input_ids, Token_type_ids, Attention_mask])\n",
    "\n",
    "\t# Use argmax along the appropriate axis to get the predicted labels\n",
    "\tpred_labels = tf.argmax(prediction.logits, axis=1)\n",
    "\n",
    "\t# Convert the TensorFlow tensor to a NumPy array and then to a list to get the predicted sentiment labels\n",
    "\tpred_labels = [label[i] for i in pred_labels.numpy().tolist()]\n",
    "\treturn pred_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Review ='''Bahubali is a blockbuster Indian movie that was released in 2015. \n",
    "It is the first part of a two-part epic saga that tells the story of a legendary hero who fights for his kingdom and his love. \n",
    "The movie has received rave reviews from critics and audiences alike for its stunning visuals, \n",
    "spectacular action scenes, and captivating storyline.'''\n",
    "Get_sentiment(Review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
